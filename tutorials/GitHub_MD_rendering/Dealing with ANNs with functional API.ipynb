{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What?** Dealing with ANNs with functional API\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For the particular case that we are dealing with, an image consisting of 28 x 28 grayscale pixels, we first need\n",
    "to read from the image and convert it into a tensor using a transforms.ToTensor() transform. We then make the \n",
    "mean and standard deviation of the pixel values 0.5 and 0.5 respectively so that it becomes easier for the model \n",
    "to train; to do this, we use transforms.Normalize((0.5,),(0.5,)). We combine all of the transformations together \n",
    "with transform.Compose().\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will be using the Fashion–MNIST dataset. This is a dataset of Zalando's article images, consisting of a \n",
    "training set of 60,000 examples and a test set of 10,000 examples. We will take an individual grayscale \n",
    "image 28 x 28 in size and convert it into a vector of 784.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.FashionMNIST('Fashion_MNIST/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testset = datasets.FashionMNIST('Fashion_MNIST/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We could define the model class with any name, but what is important is that it is a subclass of nn.Module and\n",
    "has super().__init__(), which provides the model with a lot of useful methods and attributes and retains \n",
    "knowledge of the architecture.\n",
    "\n",
    "nn.Linear describes a fully connected\n",
    "784 -> 256 -> 128 ->10\n",
    "\n",
    "We can define the network architecture without defining a network class using the nn.Sequential module, \n",
    "and it is important to ensure that the sequence of operation in the forward method is ordered properly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionNetwork(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Describes the architecture for the fashion-MINST dataset\n",
    "    The nn.Module automatically creates the weight and bias \n",
    "    tensors that we'll use in the forward method.     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialisation of the elemnts. They can be initialise in any order\n",
    "        The linear unit by itself defines a linear function, such as xW + B; \n",
    "        to have nonlinear capabilities, we need to insert nonlinear activation \n",
    "        functions, and here we use one of the most popular activation functions, ReLU.\n",
    "        \"\"\"\n",
    "        super().__init__()        \n",
    "        self.hidden1 = nn.Linear(784, 256)\n",
    "        self.hidden2 = nn.Linear(256, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "        # dim = 1, ensures that logsoftmax is taken across the columns of the output\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.hidden2(x)\n",
    "        # RELU because there are 10 output class\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "        output = self.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionNetwork(nn.Module):       \n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.hidden1 = nn.Linear(784, 256)\n",
    "        self.hidden2 = nn.Linear(256, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.hidden2(x)        \n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "        output = self.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FashionNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionNetwork(\n",
      "  (hidden1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (hidden2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0278, -0.0343, -0.0317,  ..., -0.0337, -0.0053,  0.0335],\n",
       "        [-0.0080, -0.0127,  0.0107,  ...,  0.0160, -0.0034, -0.0128],\n",
       "        [-0.0190, -0.0076,  0.0054,  ...,  0.0203,  0.0209, -0.0223],\n",
       "        ...,\n",
       "        [-0.0176, -0.0125,  0.0177,  ..., -0.0221,  0.0114,  0.0014],\n",
       "        [-0.0318,  0.0170, -0.0219,  ...,  0.0044,  0.0261,  0.0231],\n",
       "        [ 0.0220,  0.0277, -0.0186,  ...,  0.0126, -0.0341,  0.0317]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inpsecting the weights\n",
    "model.hidden1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In PyTorch, the loss function is called a criterion, and so we named our\n",
    "loss. “The log would ensure that we are not dealing with very small values\n",
    "between 0 and 1, and negative values would ensure that a logarithm of \n",
    "probability that is less than 1 is nonzero. Our goal would be to reduce this\n",
    "negative log loss error function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLLLoss()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Other optimizer functions, such as Adadelta, Adagrad, SGD.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.001,\n",
       " 'betas': (0.9, 0.999),\n",
       " 'eps': 1e-08,\n",
       " 'weight_decay': 0,\n",
       " 'amsgrad': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.003,\n",
       " 'betas': (0.9, 0.999),\n",
       " 'eps': 1e-08,\n",
       " 'weight_decay': 0,\n",
       " 'amsgrad': False}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One epoch sees all the images in the training set\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch No:  1\n",
      "Training loss: 0.2109\n",
      "Epoch No:  2\n",
      "Training loss: 0.2092\n",
      "Epoch No:  3\n",
      "Training loss: 0.2018\n",
      "Epoch No:  4\n",
      "Training loss: 0.2020\n",
      "Epoch No:  5\n",
      "Training loss: 0.1941\n",
      "Epoch No:  6\n",
      "Training loss: 0.1930\n",
      "Epoch No:  7\n",
      "Training loss: 0.1943\n",
      "Epoch No:  8\n",
      "Training loss: 0.1922\n",
      "Epoch No:  9\n",
      "Training loss: 0.1863\n",
      "Epoch No:  10\n",
      "Training loss: 0.1833\n"
     ]
    }
   ],
   "source": [
    "epochNo = 0\n",
    "for _ in range(epoch):\n",
    "    epochNo +=1\n",
    "    running_loss = 0\n",
    "    for image, label in trainloader:\n",
    "        # Zeroing the gradient because pytroch accumulated the gradients\n",
    "        # on each backward pass\n",
    "        optimizer.zero_grad()\n",
    "        # reshape each batch of 64 images\n",
    "        # from “64 x 28 x 28 to 64 x 784”\n",
    "        image = image.view(image.shape[0],-1)\n",
    "        # get a prediction\n",
    "        pred = model(image)\n",
    "        # calculate the loss\n",
    "        loss = criterion(pred, label)\n",
    "        # propagate the error backward\n",
    "        # partial derivative of the error wrt the weigths\n",
    "        loss.backward()\n",
    "        # we update the weights\n",
    "        optimizer.step()\n",
    "        # keep track of the loss\n",
    "        # the .item() method pulled a scalar out of the tensor\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch No: \", epochNo)\n",
    "        print(f'Training loss: {running_loss/len(trainloader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We add the dropout layer with a dropout probability of 0.25, which means that 25% \n",
    "of the neurons in the layer where this dropout is applied will be turned off randomly. \n",
    "Then, we edited our forward function, applied it to the first hidden layer with 256 \n",
    "units in it, and then we applied the dropout on the second layer, which has 128 units. \n",
    "We applied the activation in both the layers after going through the activation functions.\n",
    "We have to keep in mind that dropouts must be applied only on hidden layers in order to \n",
    "prevent us from losing the input data and missing outputs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(784, 256)\n",
    "        self.hidden2 = nn.Linear(256, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "        self.log_softmax = nn.LogSoftmax()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.25)\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.output(x)\n",
    "        output = self.log_softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FashionNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionNetwork(\n",
       "  (hidden1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (hidden2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (log_softmax): LogSoftmax(dim=None)\n",
       "  (activation): ReLU()\n",
       "  (drop): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing functional APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this recipe, we defined the exact same network as before, but replaced the activation\n",
    "function and the log softmax with function.relu and function.log_softmax, which makes our \n",
    "code look a lot cleaner and more concise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(784,256)\n",
    "        self.hidden2 = nn.Linear(256,128)\n",
    "        self.output = nn.Linear(128,10)\n",
    "        # This two lines are removed\n",
    "        #self.log_softmax = nn.LogSoftmax()\n",
    "        #self.activation = nn.ReLU()\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "        output = self.log_softmax(x)\n",
    "        return output\n",
    "    \"\"\"\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.log_softmax(self.output(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(784,256)\n",
    "        self.hidden2 = nn.Linear(256,128)\n",
    "        self.output = nn.Linear(128,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.log_softmax(self.output(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An EASIER way to understand when to use functional and when NOT to use is the following:\n",
    "\n",
    "As a result, layers with parameters are usually initialized in init to be shared by the whole module, while \n",
    "some connections or simple operations without parameters can be defined in forward to be used in forward \n",
    "propagation. torch.nn module is used mainly for methods which have learnable parameters, whereas\n",
    "functional is used for methods which do not have learnable parameters.\n",
    "\n",
    "A nn.Module is actually a OO wrapper around the functional interface, that contains a number of utility methods,\n",
    "like eval() and parameters(), and it automatically creates the parameters of the modules for you.\n",
    "\n",
    "Reference: https://discuss.pytorch.org/t/how-to-choose-between-torch-nn-functional-and-torch-nn-module/2800/2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Jibin Mathew, PyTorch Artificial Intelligence Fundamentals  \n",
    "- https://github.com/jibinmathew69\n",
    "    \n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
